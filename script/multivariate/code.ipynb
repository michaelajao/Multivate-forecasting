{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lagged features for 'newAdmissions' to capture trends and seasonality\n",
    "for lag in [1, 7, 14]:  # 1 day, 1 week, and 2 weeks lag\n",
    "    merged_data_corrected[f'newAdmissions_lag{lag}'] = merged_data_corrected.groupby('areaName')['newAdmissions'].shift(lag)\n",
    "\n",
    "# Creating temporal features\n",
    "merged_data_corrected['day_of_week'] = merged_data_corrected['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "merged_data_corrected['month'] = merged_data_corrected['date'].dt.month  # January=1, December=12\n",
    "merged_data_corrected['week_of_year'] = merged_data_corrected['date'].dt.isocalendar().week\n",
    "\n",
    "# Assuming population density and healthcare infrastructure data are available and merged into the dataset\n",
    "# For demonstration, we'll use existing 'population' as a proxy for spatial features\n",
    "# If additional spatial features were available, they could be directly included in the dataframe\n",
    "\n",
    "# Dropping any rows with NaN values that may have been introduced by creating lagged features\n",
    "merged_data_corrected.dropna(inplace=True)\n",
    "\n",
    "# Displaying the updated dataframe to verify the new features\n",
    "merged_data_corrected[['date', 'areaName', 'newAdmissions', 'newAdmissions_lag1', 'newAdmissions_lag7', 'newAdmissions_lag14', 'day_of_week', 'month', 'week_of_year']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=30, target_delay=1, features=[], target='newAdmissions'):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_delay = target_delay\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.data[self.features])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length - self.target_delay + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx\n",
    "        end = idx + self.sequence_length\n",
    "        data = self.data.iloc[start:end]\n",
    "        x = self.scaler.transform(data[self.features].values)\n",
    "        y = self.data.iloc[end + self.target_delay - 1][self.target]\n",
    "        return torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# Specify features and target\n",
    "features = ['newAdmissions_lag1', 'newAdmissions_lag7', 'newAdmissions_lag14', 'day_of_week', 'month', 'week_of_year']\n",
    "target = 'newAdmissions'\n",
    "\n",
    "# Create the dataset\n",
    "sequence_length = 30  # Example sequence length\n",
    "target_delay = 1  # Predicting the next day\n",
    "dataset = TimeseriesDataset(merged_data_corrected, sequence_length, target_delay, features, target)\n",
    "\n",
    "# Splitting the data (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "val_size = int(len(dataset) * 0.15)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "class GraphCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_node_features, num_nodes, lstm_hidden_size, output_size):\n",
    "        super(GraphCNNLSTM, self).__init__()\n",
    "        # Graph convolutional layer\n",
    "        self.graph_conv = geom_nn.GCNConv(num_node_features, 16)\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=16*num_nodes, hidden_size=lstm_hidden_size, batch_first=True)\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch_data):\n",
    "        # x: Node features, shape [num_nodes, num_node_features]\n",
    "        # edge_index: Graph connectivity, shape [2, num_edges]\n",
    "        \n",
    "        # Spatial processing with graph convolution\n",
    "        x = self.graph_conv(x, edge_index)\n",
    "        x = x.view(batch_data.size(0), -1)  # Reshape for LSTM input\n",
    "        \n",
    "        # Temporal processing with LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(batch_data)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Use the last LSTM output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inverse transformation for predictions and actuals\n",
    "def inverse_transform_data(scaler, data, feature_order):\n",
    "    # Create a placeholder for the full feature set based on the scaler's expected input\n",
    "    full_data = np.zeros((data.shape[0], scaler.n_features_in_))\n",
    "\n",
    "    # Fill in the placeholder with the actual data\n",
    "    for i, column in enumerate(feature_order):\n",
    "        full_data[:, i] = data[:, column]\n",
    "\n",
    "    # Perform the inverse transformation\n",
    "    return scaler.inverse_transform(full_data)\n",
    "\n",
    "# Function to plot actual vs predicted data after inverse transformation\n",
    "def plot_actual_vs_predicted(time_points, actual, predicted, labels, title, filename=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(time_points, actual[:, i], label=f'{label} Actual', linewidth=2)\n",
    "        plt.plot(time_points, predicted[:, i], '--', label=f'{label} Predicted', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Days since: 2020-04-01\")\n",
    "    plt.ylabel(\"Population\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "# Extract the time points from t_tensor for plotting\n",
    "time_points = t_data.cpu().detach().numpy()\n",
    "\n",
    "# Actual values\n",
    "actual_SIR = np.array([S_actual, I_actual, R_actual]).T\n",
    "\n",
    "# Predicted values\n",
    "predicted_SIR = np.array([S_pred, I_pred, R_pred]).T\n",
    "\n",
    "# Inverse transform the actual and predicted values\n",
    "actual_SIR_transformed = inverse_transform_data(transformer, actual_SIR, [2, 1, 0])  # Susceptible, Infected, Recovered\n",
    "predicted_SIR_transformed = inverse_transform_data(transformer, predicted_SIR, [2, 1, 0])\n",
    "\n",
    "# Plot actual vs predicted\n",
    "labels = ['Susceptible', 'Infected', 'Recovered']\n",
    "plot_actual_vs_predicted(time_points, actual_SIR_transformed, predicted_SIR_transformed, labels, \"SIR Model Predictions vs. Actual Data\", filename=\"../../images/sir_model_predictions_inverse_transformed.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
