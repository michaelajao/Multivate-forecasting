{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.0.1+cu117\n",
      "CUDA available: True\n",
      "CUDA version: 11.7\n",
      "Available GPUs: 2\n",
      "GPU 0: Quadro RTX 8000\n",
      "GPU 1: Quadro RTX 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tanh(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 321\u001b[0m\n\u001b[1;32m    318\u001b[0m data_tensors \u001b[38;5;241m=\u001b[39m (S_data, I_data, R_data, D_data)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    324\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[2], line 238\u001b[0m, in \u001b[0;36mpinn_loss\u001b[0;34m(t, data, state_nn, param_nn, N, sigma, alpha, epsilon)\u001b[0m\n\u001b[1;32m    235\u001b[0m beta_pred, gamma_pred, mu_pred \u001b[38;5;241m=\u001b[39m param_nn\u001b[38;5;241m.\u001b[39mpredict_parameters(t)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# SEIRD model residuals\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(param_nn\u001b[38;5;241m.\u001b[39malpha)\n\u001b[1;32m    241\u001b[0m dSdt, dEdt, dIdt, dRdt, dDdt \u001b[38;5;241m=\u001b[39m SEIRD_model(t, [S_pred, E_pred, I_pred, R_pred, D_pred], beta_pred, gamma_pred, mu_pred, sigma, e, alpha, N)\n",
      "\u001b[0;31mTypeError\u001b[0m: tanh(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set matplotlib style and parameters\n",
    "plt.style.use(\"seaborn-v0_8-poster\")\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 20,\n",
    "    \"figure.figsize\": [10, 5],\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"figure.autolayout\": True,\n",
    "    \"figure.dpi\": 600,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"savefig.format\": \"pdf\",\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.formatter.limits\": (0, 5),\n",
    "    \"axes.formatter.use_mathtext\": True,\n",
    "    \"axes.formatter.useoffset\": False,\n",
    "    \"axes.xmargin\": 0,\n",
    "    \"axes.ymargin\": 0,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"legend.frameon\": False,\n",
    "    \"legend.loc\": \"best\",\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markersize\": 8,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"xtick.direction\": \"in\",\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"ytick.direction\": \"in\",\n",
    "    \"ytick.right\": False,\n",
    "    \"grid.color\": \"grey\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"errorbar.capsize\": 4,\n",
    "    \"figure.subplot.wspace\": 0.4,\n",
    "    \"figure.subplot.hspace\": 0.4,\n",
    "    \"image.cmap\": \"viridis\",\n",
    "})\n",
    "\n",
    "# Device setup for CUDA or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def check_pytorch():\n",
    "    \"\"\"Check PyTorch and CUDA setup.\"\"\"\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "    if cuda_available:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"Available GPUs: {gpu_count}\")\n",
    "        for i in range(gpu_count):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"CUDA not available. PyTorch will run on CPU.\")\n",
    "\n",
    "check_pytorch()\n",
    "\n",
    "def load_and_preprocess_data(filepath, areaname, recovery_period=16, rolling_window=7, start_date=\"2020-04-01\", end_date=\"2020-12-31\"):\n",
    "    \"\"\"Load and preprocess the data from a CSV file.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[df[\"areaName\"] == areaname].reset_index(drop=True)\n",
    "    df = df[::-1].reset_index(drop=True)  # Reverse dataset if needed\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[(df[\"date\"] >= pd.to_datetime(start_date)) & (df[\"date\"] <= pd.to_datetime(end_date))]\n",
    "\n",
    "    df[\"recovered\"] = df[\"cumulative_confirmed\"].shift(recovery_period) - df[\"cumulative_deceased\"].shift(recovery_period)\n",
    "    df[\"recovered\"] = df[\"recovered\"].fillna(0).clip(lower=0)\n",
    "    df[\"active_cases\"] = df[\"cumulative_confirmed\"] - df[\"recovered\"] - df[\"cumulative_deceased\"]\n",
    "    df[\"S(t)\"] = df[\"population\"] - df[\"cumulative_confirmed\"] - df[\"cumulative_deceased\"] - df[\"recovered\"]\n",
    "\n",
    "    cols_to_smooth = [\"S(t)\",\"cumulative_confirmed\", \"cumulative_deceased\", \"hospitalCases\", \"covidOccupiedMVBeds\", \"recovered\", \"active_cases\", \"new_deceased\", \"new_confirmed\"]\n",
    "    for col in cols_to_smooth:\n",
    "        df[col] = df[col].rolling(window=rolling_window, min_periods=1).mean().fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = load_and_preprocess_data(\"../../data/hos_data/merged_data.csv\", areaname=\"South West\", recovery_period=21, rolling_window=7, start_date=\"2020-04-01\", end_date=\"2020-12-31\").drop(columns=[\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data.head(10)\n",
    "\n",
    "def SEIRD_model(t, y, beta, gamma, mu, sigma, e, alpha, N):\n",
    "    \"\"\"SEIRD model differential equations.\"\"\"\n",
    "    S, E, I, R, D = y\n",
    "    dSdt = -beta * S * (e * E + I) / N\n",
    "    dEdt = beta * S * (e * E + I) / N - E / alpha\n",
    "    dIdt = E / alpha - (gamma + mu) * I\n",
    "    dRdt = gamma * I\n",
    "    dDdt = mu * I\n",
    "    return [dSdt, dEdt, dIdt, dRdt, dDdt]\n",
    "\n",
    "def prepare_tensors(data, device):\n",
    "    \"\"\"Prepare tensors for training.\"\"\"\n",
    "    t = tensor(range(1, len(data) + 1), dtype=torch.float32).view(-1, 1).to(device).requires_grad_(True)\n",
    "    S = tensor(data[\"S(t)\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    I = tensor(data[\"active_cases\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    R = tensor(data[\"recovered\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    D = tensor(data[\"new_deceased\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    return t, S, I, R, D\n",
    "\n",
    "def scale_data(data, features):\n",
    "    \"\"\"Scale the data using MinMaxScaler.\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data[features]), columns=features)\n",
    "    return scaled_data, scaler  \n",
    "\n",
    "# Define features and data split\n",
    "features = [\"S(t)\", \"active_cases\", \"recovered\", \"new_deceased\"]\n",
    "\n",
    "# Scale the data\n",
    "scaled_data, scaler = scale_data(data, features)\n",
    "\n",
    "# Prepare tensors\n",
    "t_data, S_data, I_data, R_data, D_data = prepare_tensors(scaled_data, device)\n",
    "\n",
    "class ModifiedTanh(nn.Module):\n",
    "    def __init__(self, alpha, epsilon):\n",
    "        super(ModifiedTanh, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * torch.tanh(self.alpha * x) + self.epsilon\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        out = self.fc(x)\n",
    "        out = self.activation(out)\n",
    "        if out.shape == identity.shape:\n",
    "            out = out + identity\n",
    "        return out\n",
    "\n",
    "class StateNN(nn.Module):\n",
    "    \"\"\"Epidemiological network for predicting SEIRD model outputs.\"\"\"\n",
    "    def __init__(self, num_layers=4, hidden_neurons=20):\n",
    "        super(StateNN, self).__init__()\n",
    "        layers = [nn.Linear(1, hidden_neurons), nn.Tanh()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(ResBlock(hidden_neurons, hidden_neurons))\n",
    "        layers.append(nn.Linear(hidden_neurons, 5))  # Adjust the output size to 5 (S, E, I, R, D)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "class ParamNN(nn.Module):\n",
    "    \"\"\"Neural network for predicting time-varying parameters.\"\"\"\n",
    "    def __init__(self, num_layers=4, hidden_neurons=20, alpha=1.0, epsilon=0.0):\n",
    "        super(ParamNN, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        layers = [nn.Linear(1, hidden_neurons), ModifiedTanh(alpha, epsilon)]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(ResBlock(hidden_neurons, hidden_neurons))\n",
    "        layers.append(nn.Linear(hidden_neurons, 3))  # Adjust the output size to 3 (beta, gamma, mu)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "    def predict_parameters(self, t):\n",
    "        params = self.net(t)\n",
    "        # Ensure beta, gamma, and mu are in a valid range\n",
    "        beta = torch.sigmoid(params[:, 0]) * 0.5  # range: [0, 0.5]\n",
    "        gamma = torch.sigmoid(params[:, 1]) * 0.1  # range: [0, 0.1]\n",
    "        mu = torch.sigmoid(params[:, 2]) * 0.1  # range: [0, 0.1]\n",
    "        return beta, gamma, mu\n",
    "\n",
    "def pinn_loss(t, data, state_nn, param_nn, N, sigma, alpha, epsilon):\n",
    "    \"\"\"Physics-Informed Neural Network loss function.\"\"\"\n",
    "    \n",
    "    # Predicted states\n",
    "    states_pred = state_nn(t)\n",
    "    S_pred, E_pred, I_pred, R_pred, D_pred = states_pred[:, 0], states_pred[:, 1], states_pred[:, 2], states_pred[:, 3], states_pred[:, 4]\n",
    "    \n",
    "    # Compute gradients\n",
    "    S_t = grad(S_pred, t, grad_outputs=torch.ones_like(S_pred), create_graph=True)[0]\n",
    "    E_t = grad(E_pred, t, grad_outputs=torch.ones_like(E_pred), create_graph=True)[0]   \n",
    "    I_t = grad(I_pred, t, grad_outputs=torch.ones_like(I_pred), create_graph=True)[0]\n",
    "    R_t = grad(R_pred, t, grad_outputs=torch.ones_like(R_pred), create_graph=True)[0]\n",
    "    D_t = grad(D_pred, t, grad_outputs=torch.ones_like(D_pred), create_graph=True)[0]\n",
    "    \n",
    "    # Predicted parameters\n",
    "    beta_pred, gamma_pred, mu_pred = param_nn.predict_parameters(t)\n",
    "    \n",
    "    # SEIRD model residuals\n",
    "    e = torch.tanh(param_nn.epsilon)\n",
    "    alpha = 2 * torch.tanh(param_nn.alpha)\n",
    "    \n",
    "    dSdt, dEdt, dIdt, dRdt, dDdt = SEIRD_model(t, [S_pred, E_pred, I_pred, R_pred, D_pred], beta_pred, gamma_pred, mu_pred, sigma, e, alpha, N)\n",
    "    \n",
    "    # Compute data loss (MSE_u)\n",
    "    S_data, I_data, R_data, D_data = data\n",
    "    loss_data = torch.mean((S_pred - S_data)**2) + torch.mean((I_pred - I_data)**2) + torch.mean((R_pred - R_data)**2) + torch.mean((D_pred - D_data)**2)\n",
    "    \n",
    "    # Compute physics loss (MSE_f)\n",
    "    loss_physics = torch.mean((S_t - dSdt)**2) + torch.mean((E_t - dEdt)**2) + torch.mean((I_t - dIdt)**2) + torch.mean((R_t - dRdt)**2) + torch.mean((D_t - dDdt)**2)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = loss_data + loss_physics\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.counter = 0\n",
    "        self.loss_history = deque(maxlen=patience + 1)\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50000\n",
    "sigma = 1/5\n",
    "N = data[\"population\"].values[0]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "\n",
    "# Instantiate the neural networks with custom activation function\n",
    "state_nn = StateNN(num_layers=6, hidden_neurons=32).to(device)\n",
    "param_nn = ParamNN(num_layers=6, hidden_neurons=32, alpha=alpha, epsilon=epsilon).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_state = optim.Adam(state_nn.parameters(), lr=learning_rate)\n",
    "optimizer_param = optim.Adam(param_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping criteria\n",
    "early_stopping = EarlyStopping(patience=20, verbose=False)\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    state_nn.train()\n",
    "    param_nn.train()\n",
    "    \n",
    "    optimizer_state.zero_grad()\n",
    "    optimizer_param.zero_grad()\n",
    "    \n",
    "    # Prepare time tensor\n",
    "    t = torch.linspace(0, 1, steps=len(data)).view(-1, 1).to(device).requires_grad_(True)\n",
    "    \n",
    "    # Prepare data tensor\n",
    "    S_data = torch.tensor(data[\"S(t)\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    I_data = torch.tensor(data[\"active_cases\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    R_data = torch.tensor(data[\"recovered\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    D_data = torch.tensor(data[\"new_deceased\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    data_tensors = (S_data, I_data, R_data, D_data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = pinn_loss(t, data_tensors, state_nn, param_nn, N, sigma, alpha, epsilon)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer_state.step()\n",
    "    optimizer_param.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    if early_stopping(loss.item()):\n",
    "        print(f\"Early stopping at epoch {epoch}. No improvement in loss for {early_stopping.patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.log10(loss_history), label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict and plot the results\n",
    "state_nn.eval()\n",
    "param_nn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, steps=len(t_data)).view(-1, 1).to(device)\n",
    "    states_pred = state_nn(t).cpu().numpy()\n",
    "    S_pred, E_pred, I_pred, R_pred, D_pred = states_pred[:, 0], states_pred[:, 1], states_pred[:, 2], states_pred[:, 3], states_pred[:, 4]\n",
    "\n",
    "    # Inverse transform to get back to original scale for each variable separately\n",
    "    I_pred = scaler.inverse_transform(np.concatenate([I_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "    R_pred = scaler.inverse_transform(np.concatenate([R_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "    D_pred = scaler.inverse_transform(np.concatenate([D_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "\n",
    "# Plot infected predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], I_pred, label='Infected (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"active_cases\"], label='Infected (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Infected Population')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot recovered predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], R_pred, label='Recovered (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"recovered\"], label='Recovered (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Recovered Population')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot deceased predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], D_pred, label='Deceased (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"new_deceased\"], label='Deceased (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Deceased Population')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.0.1+cu117\n",
      "CUDA available: True\n",
      "CUDA version: 11.7\n",
      "Available GPUs: 2\n",
      "GPU 0: Quadro RTX 8000\n",
      "GPU 1: Quadro RTX 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'E_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 317\u001b[0m\n\u001b[1;32m    314\u001b[0m data_tensors \u001b[38;5;241m=\u001b[39m (S_data, I_data, R_data, D_data)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    320\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[4], line 249\u001b[0m, in \u001b[0;36mpinn_loss\u001b[0;34m(t, data, state_nn, param_nn, N, sigma, alpha, epsilon)\u001b[0m\n\u001b[1;32m    246\u001b[0m loss_physics \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((S_t \u001b[38;5;241m-\u001b[39m dSdt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((E_t \u001b[38;5;241m-\u001b[39m dEdt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((I_t \u001b[38;5;241m-\u001b[39m dIdt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((R_t \u001b[38;5;241m-\u001b[39m dRdt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((D_t \u001b[38;5;241m-\u001b[39m dDdt)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# initial condition loss\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m loss_initial \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((S_pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m S_data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((E_pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mE_data\u001b[49m[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((I_pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m I_data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((R_pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m R_data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((D_pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m D_data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)   \n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Total loss    \u001b[39;00m\n\u001b[1;32m    252\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_data \u001b[38;5;241m+\u001b[39m loss_physics \u001b[38;5;241m+\u001b[39m loss_initial\n",
      "\u001b[0;31mNameError\u001b[0m: name 'E_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set matplotlib style and parameters\n",
    "plt.style.use(\"seaborn-v0_8-poster\")\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 20,\n",
    "    \"figure.figsize\": [10, 5],\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"figure.autolayout\": True,\n",
    "    \"figure.dpi\": 600,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"savefig.format\": \"pdf\",\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.formatter.limits\": (0, 5),\n",
    "    \"axes.formatter.use_mathtext\": True,\n",
    "    \"axes.formatter.useoffset\": False,\n",
    "    \"axes.xmargin\": 0,\n",
    "    \"axes.ymargin\": 0,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"legend.frameon\": False,\n",
    "    \"legend.loc\": \"best\",\n",
    "    \"lines.linewidth\": 2,\n",
    "    \"lines.markersize\": 8,\n",
    "    \"xtick.labelsize\": 14,\n",
    "    \"xtick.direction\": \"in\",\n",
    "    \"xtick.top\": False,\n",
    "    \"ytick.labelsize\": 14,\n",
    "    \"ytick.direction\": \"in\",\n",
    "    \"ytick.right\": False,\n",
    "    \"grid.color\": \"grey\",\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"errorbar.capsize\": 4,\n",
    "    \"figure.subplot.wspace\": 0.4,\n",
    "    \"figure.subplot.hspace\": 0.4,\n",
    "    \"image.cmap\": \"viridis\",\n",
    "})\n",
    "\n",
    "# Device setup for CUDA or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def check_pytorch():\n",
    "    \"\"\"Check PyTorch and CUDA setup.\"\"\"\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "    if cuda_available:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"Available GPUs: {gpu_count}\")\n",
    "        for i in range(gpu_count):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"CUDA not available. PyTorch will run on CPU.\")\n",
    "\n",
    "check_pytorch()\n",
    "\n",
    "def load_and_preprocess_data(filepath, areaname, recovery_period=16, rolling_window=7, start_date=\"2020-04-01\", end_date=\"2020-12-31\"):\n",
    "    \"\"\"Load and preprocess the data from a CSV file.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df[df[\"areaName\"] == areaname].reset_index(drop=True)\n",
    "    df = df[::-1].reset_index(drop=True)  # Reverse dataset if needed\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df[(df[\"date\"] >= pd.to_datetime(start_date)) & (df[\"date\"] <= pd.to_datetime(end_date))]\n",
    "\n",
    "    df[\"recovered\"] = df[\"cumulative_confirmed\"].shift(recovery_period) - df[\"cumulative_deceased\"].shift(recovery_period)\n",
    "    df[\"recovered\"] = df[\"recovered\"].fillna(0).clip(lower=0)\n",
    "    df[\"active_cases\"] = df[\"cumulative_confirmed\"] - df[\"recovered\"] - df[\"cumulative_deceased\"]\n",
    "    df[\"S(t)\"] = df[\"population\"] - df[\"cumulative_confirmed\"] - df[\"cumulative_deceased\"] - df[\"recovered\"]\n",
    "\n",
    "    cols_to_smooth = [\"S(t)\", \"cumulative_confirmed\", \"cumulative_deceased\", \"hospitalCases\", \"covidOccupiedMVBeds\", \"recovered\", \"active_cases\", \"new_deceased\", \"new_confirmed\"]\n",
    "    for col in cols_to_smooth:\n",
    "        df[col] = df[col].rolling(window=rolling_window, min_periods=1).mean().fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = load_and_preprocess_data(\"../../data/hos_data/merged_data.csv\", areaname=\"South West\", recovery_period=21, rolling_window=7, start_date=\"2020-04-01\", end_date=\"2020-12-31\").drop(columns=[\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data.head(10)\n",
    "\n",
    "def SEIRD_model(t, y, beta, gamma, mu, sigma, e, alpha, N):\n",
    "    \"\"\"SEIRD model differential equations.\"\"\"\n",
    "    S, E, I, R, D = y\n",
    "    dSdt = -beta * S * (e * E + I) / N\n",
    "    dEdt = beta * S * (e * E + I) / N - E / alpha\n",
    "    dIdt = E / alpha - (gamma + mu) * I\n",
    "    dRdt = gamma * I\n",
    "    dDdt = mu * I\n",
    "    return [dSdt, dEdt, dIdt, dRdt, dDdt]\n",
    "\n",
    "def prepare_tensors(data, device):\n",
    "    \"\"\"Prepare tensors for training.\"\"\"\n",
    "    t = tensor(range(1, len(data) + 1), dtype=torch.float32).view(-1, 1).to(device).requires_grad_(True)\n",
    "    S = tensor(data[\"S(t)\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    I = tensor(data[\"active_cases\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    R = tensor(data[\"recovered\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    D = tensor(data[\"new_deceased\"].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    return t, S, I, R, D\n",
    "\n",
    "def scale_data(data, features):\n",
    "    \"\"\"Scale the data using MinMaxScaler.\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data[features]), columns=features)\n",
    "    return scaled_data, scaler  \n",
    "\n",
    "# Define features and data split\n",
    "features = [\"S(t)\", \"active_cases\", \"recovered\", \"new_deceased\"]\n",
    "\n",
    "# Scale the data\n",
    "scaled_data, scaler = scale_data(data, features)\n",
    "\n",
    "# Prepare tensors\n",
    "t_data, S_data, I_data, R_data, D_data = prepare_tensors(scaled_data, device)\n",
    "\n",
    "class ModifiedTanh(nn.Module):\n",
    "    def __init__(self, alpha, epsilon):\n",
    "        super(ModifiedTanh, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * torch.tanh(self.alpha * x) + self.epsilon\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        out = self.fc(x)\n",
    "        out = self.activation(out)\n",
    "        if out.shape == identity.shape:\n",
    "            out = out + identity\n",
    "        return out\n",
    "\n",
    "class StateNN(nn.Module):\n",
    "    \"\"\"Epidemiological network for predicting SEIRD model outputs.\"\"\"\n",
    "    def __init__(self, num_layers=4, hidden_neurons=20):\n",
    "        super(StateNN, self).__init__()\n",
    "        layers = [nn.Linear(1, hidden_neurons), nn.Tanh()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(ResBlock(hidden_neurons, hidden_neurons))\n",
    "        layers.append(nn.Linear(hidden_neurons, 5))  # Adjust the output size to 5 (S, E, I, R, D)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "class ParamNN(nn.Module):\n",
    "    \"\"\"Neural network for predicting time-varying parameters.\"\"\"\n",
    "    def __init__(self, num_layers=4, hidden_neurons=20):\n",
    "        super(ParamNN, self).__init__()\n",
    "        layers = [nn.Linear(1, hidden_neurons), nn.Tanh()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(ResBlock(hidden_neurons, hidden_neurons))\n",
    "        layers.append(nn.Linear(hidden_neurons, 3))  # Adjust the output size to 3 (beta, gamma, mu)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        params = self.net(t)\n",
    "        # Ensure beta, gamma, and mu are in a valid range\n",
    "        beta = torch.sigmoid(params[:, 0]) * 0.5  # range: [0, 0.5]\n",
    "        gamma = torch.sigmoid(params[:, 1]) * 0.1  # range: [0, 0.1]\n",
    "        mu = torch.sigmoid(params[:, 2]) * 0.1  # range: [0, 0.1]\n",
    "        return beta, gamma, mu\n",
    "\n",
    "def pinn_loss(t, data, state_nn, param_nn, N, sigma, alpha, epsilon):\n",
    "    \"\"\"Physics-Informed Neural Network loss function.\"\"\"\n",
    "    \n",
    "    # Predicted states\n",
    "    states_pred = state_nn(t)\n",
    "    S_pred, E_pred, I_pred, R_pred, D_pred = states_pred[:, 0], states_pred[:, 1], states_pred[:, 2], states_pred[:, 3], states_pred[:, 4]\n",
    "    \n",
    "    # Compute gradients\n",
    "    S_t = grad(S_pred, t, grad_outputs=torch.ones_like(S_pred), create_graph=True)[0]\n",
    "    E_t = grad(E_pred, t, grad_outputs=torch.ones_like(E_pred), create_graph=True)[0]   \n",
    "    I_t = grad(I_pred, t, grad_outputs=torch.ones_like(I_pred), create_graph=True)[0]\n",
    "    R_t = grad(R_pred, t, grad_outputs=torch.ones_like(R_pred), create_graph=True)[0]\n",
    "    D_t = grad(D_pred, t, grad_outputs=torch.ones_like(D_pred), create_graph=True)[0]\n",
    "    \n",
    "    # Predicted parameters\n",
    "    beta_pred, gamma_pred, mu_pred = param_nn(t)\n",
    "    \n",
    "    # SEIRD model residuals\n",
    "    e_tensor = torch.tensor(epsilon, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    alpha_tensor = torch.tensor(alpha, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    \n",
    "    e = torch.tanh(e_tensor)\n",
    "    alpha = 2 * torch.tanh(alpha_tensor)\n",
    "    \n",
    "    dSdt, dEdt, dIdt, dRdt, dDdt = SEIRD_model(t, [S_pred, E_pred, I_pred, R_pred, D_pred], beta_pred, gamma_pred, mu_pred, sigma, e, alpha, N)\n",
    "    \n",
    "    # Compute data loss (MSE_u)\n",
    "    S_data, I_data, R_data, D_data = data\n",
    "    loss_data = torch.mean((S_pred - S_data)**2) + torch.mean((I_pred - I_data)**2) + torch.mean((R_pred - R_data)**2) + torch.mean((D_pred - D_data)**2)\n",
    "    \n",
    "    # Compute physics loss (MSE_f)\n",
    "    loss_physics = torch.mean((S_t - dSdt)**2) + torch.mean((E_t - dEdt)**2) + torch.mean((I_t - dIdt)**2) + torch.mean((R_t - dRdt)**2) + torch.mean((D_t - dDdt)**2)\n",
    "    \n",
    "    # initial condition loss\n",
    "    loss_initial = torch.mean((S_pred[0] - S_data[0])**2) + torch.mean((I_pred[0] - I_data[0])**2) + torch.mean((R_pred[0] - R_data[0])**2) + torch.mean((D_pred[0] - D_data[0])**2)\n",
    "    \n",
    "    # Total loss    \n",
    "    total_loss = loss_data + loss_physics + loss_initial\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.counter = 0\n",
    "        self.loss_history = deque(maxlen=patience + 1)\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50000\n",
    "sigma = 1/5\n",
    "N = data[\"population\"].values[0]\n",
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "\n",
    "# Instantiate the neural networks with custom activation function\n",
    "state_nn = StateNN(num_layers=6, hidden_neurons=32).to(device)\n",
    "param_nn = ParamNN(num_layers=6, hidden_neurons=32).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_state = optim.Adam(state_nn.parameters(), lr=learning_rate)\n",
    "optimizer_param = optim.Adam(param_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping criteria\n",
    "early_stopping = EarlyStopping(patience=20, verbose=False)\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    state_nn.train()\n",
    "    param_nn.train()\n",
    "    \n",
    "    optimizer_state.zero_grad()\n",
    "    optimizer_param.zero_grad()\n",
    "    \n",
    "    # Prepare time tensor\n",
    "    # Prepare time tensor\n",
    "    t = t_data\n",
    "    \n",
    "    data_tensors = (S_data, I_data, R_data, D_data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = pinn_loss(t, data_tensors, state_nn, param_nn, N, sigma, alpha, epsilon)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer_state.step()\n",
    "    optimizer_param.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    if early_stopping(loss.item()):\n",
    "        print(f\"Early stopping at epoch {epoch}. No improvement in loss for {early_stopping.patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.log10(loss_history), label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict and plot the results\n",
    "state_nn.eval()\n",
    "param_nn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    t = torch.linspace(0, 1, steps=len(t_data)).view(-1, 1).to(device)\n",
    "    states_pred = state_nn(t).cpu().numpy()\n",
    "    S_pred, E_pred, I_pred, R_pred, D_pred = states_pred[:, 0], states_pred[:, 1], states_pred[:, 2], states_pred[:, 3], states_pred[:, 4]\n",
    "\n",
    "    # Inverse transform to get back to original scale for each variable separately\n",
    "    I_pred = scaler.inverse_transform(np.concatenate([I_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "    R_pred = scaler.inverse_transform(np.concatenate([R_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "    D_pred = scaler.inverse_transform(np.concatenate([D_pred.reshape(-1, 1)] * len(features), axis=1))[:, 0]\n",
    "\n",
    "# Plot infected predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], I_pred, label='Infected (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"active_cases\"], label='Infected (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Infected Population')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot recovered predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], R_pred, label='Recovered (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"recovered\"], label='Recovered (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Recovered Population')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot deceased predicted vs real data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data[\"date\"], D_pred, label='Deceased (Predicted)')\n",
    "plt.plot(data[\"date\"], data[\"new_deceased\"], label='Deceased (Actual)', linestyle='dashed')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Deceased Population')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
