{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.utils import plotting_utils\n",
    "from src.transforms.target_transformations import AutoStationaryTransformer\n",
    "import plotly.io as pio\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Description: This script contains the code for the second experiment in the project, \n",
    "# forecasting COVID-19 MVBeds using various RNN models and hyperparameter tuning with Simulated Annealing.\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "pl.seed_everything(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Set default plotly template\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and Prepare Data\n",
    "data_path = Path(\"data/processed/merged_nhs_covid_data.csv\")\n",
    "data = pd.read_csv(data_path).drop(\"Unnamed: 0\", axis=1)\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "# Select a different area name\n",
    "selected_area = \"East of England\"\n",
    "data_filtered = data[data[\"areaName\"] == selected_area]\n",
    "\n",
    "# Data Processing\n",
    "data_filtered[\"date\"] = pd.to_datetime(data_filtered[\"date\"])\n",
    "data_filtered.sort_values(by=[\"date\", \"areaName\"], inplace=True)\n",
    "data_filtered.drop(\n",
    "    [\n",
    "        \"areaName\",\n",
    "        \"areaCode\",\n",
    "        \"cumAdmissions\",\n",
    "        \"cumulative_confirmed\",\n",
    "        \"cumulative_deceased\",\n",
    "        \"population\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"epi_week\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospitalCases: hospitalCases_rolling_7_mean, hospitalCases_rolling_7_std\n",
      "newAdmissions: newAdmissions_rolling_7_mean, newAdmissions_rolling_7_std\n",
      "new_confirmed: new_confirmed_rolling_7_mean, new_confirmed_rolling_7_std\n",
      "new_deceased: new_deceased_rolling_7_mean, new_deceased_rolling_7_std\n"
     ]
    }
   ],
   "source": [
    "def add_rolling_features(df, window_size, columns, agg_funcs=None):\n",
    "    if agg_funcs is None:\n",
    "        agg_funcs = [\"mean\"]\n",
    "    added_features = {}\n",
    "    for column in columns:\n",
    "        for func in agg_funcs:\n",
    "            roll_col_name = f\"{column}_rolling_{window_size}_{func}\"\n",
    "            df[roll_col_name] = df[column].rolling(window_size).agg(func)\n",
    "            if column not in added_features:\n",
    "                added_features[column] = []\n",
    "            added_features[column].append(roll_col_name)\n",
    "    df.dropna(inplace=True)\n",
    "    return df, added_features\n",
    "\n",
    "# Configuration\n",
    "window_size = 7\n",
    "columns_to_roll = [\"hospitalCases\", \"newAdmissions\", \"new_confirmed\", \"new_deceased\"]\n",
    "agg_funcs = [\"mean\", \"std\"]\n",
    "\n",
    "# Apply rolling features for each column\n",
    "data_filtered, added_features = add_rolling_features(\n",
    "    data_filtered, window_size, columns_to_roll, agg_funcs\n",
    ")\n",
    "\n",
    "for column, features in added_features.items():\n",
    "    print(f\"{column}: {', '.join(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_lags(data, lags, features):\n",
    "    added_features = []\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            new_feature = feature + f\"_lag_{lag}\"\n",
    "            data[new_feature] = data[feature].shift(lag)\n",
    "            added_features.append(new_feature)\n",
    "    return data, added_features\n",
    "\n",
    "lags = [1, 2, 3, 5, 7, 14, 21]\n",
    "data_filtered, added_features = add_lags(data_filtered, lags, [\"covidOccupiedMVBeds\"])\n",
    "data_filtered.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df, date_column):\n",
    "    df[\"month\"] = df[date_column].dt.month\n",
    "    df[\"day\"] = df[date_column].dt.day\n",
    "    df[\"day_of_week\"] = df[date_column].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "data_filtered = create_temporal_features(data_filtered, \"date\")\n",
    "data_filtered.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 396 entries, 2020-05-01 to 2021-05-31\n",
      "Data columns (total 28 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   covidOccupiedMVBeds           396 non-null    float64\n",
      " 1   hospitalCases                 396 non-null    float64\n",
      " 2   newAdmissions                 396 non-null    int64  \n",
      " 3   new_confirmed                 396 non-null    float64\n",
      " 4   new_deceased                  396 non-null    float64\n",
      " 5   hospitalCases_rolling_7_mean  396 non-null    float64\n",
      " 6   hospitalCases_rolling_7_std   396 non-null    float64\n",
      " 7   newAdmissions_rolling_7_mean  396 non-null    float64\n",
      " 8   newAdmissions_rolling_7_std   396 non-null    float64\n",
      " 9   new_confirmed_rolling_7_mean  396 non-null    float64\n",
      " 10  new_confirmed_rolling_7_std   396 non-null    float64\n",
      " 11  new_deceased_rolling_7_mean   396 non-null    float64\n",
      " 12  new_deceased_rolling_7_std    396 non-null    float64\n",
      " 13  covidOccupiedMVBeds_lag_1     396 non-null    float64\n",
      " 14  covidOccupiedMVBeds_lag_2     396 non-null    float64\n",
      " 15  covidOccupiedMVBeds_lag_3     396 non-null    float64\n",
      " 16  covidOccupiedMVBeds_lag_5     396 non-null    float64\n",
      " 17  covidOccupiedMVBeds_lag_7     396 non-null    float64\n",
      " 18  covidOccupiedMVBeds_lag_14    396 non-null    float64\n",
      " 19  covidOccupiedMVBeds_lag_21    396 non-null    float64\n",
      " 20  month                         396 non-null    int64  \n",
      " 21  day                           396 non-null    int64  \n",
      " 22  day_of_week                   396 non-null    int64  \n",
      " 23  susceptible                   396 non-null    float64\n",
      " 24  exposed                       396 non-null    float64\n",
      " 25  active_cases                  396 non-null    float64\n",
      " 26  recovered                     396 non-null    float64\n",
      " 27  cumulative_deceased           396 non-null    float64\n",
      "dtypes: float64(24), int64(4)\n",
      "memory usage: 89.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Load and process the SEIRD data\n",
    "seird_data = pd.read_csv(f\"reports/output/pinn_{selected_area}_output.csv\")\n",
    "seird_data[\"date\"] = pd.to_datetime(seird_data[\"date\"])\n",
    "seird_data.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Merge the two dataframes on the date index\n",
    "merged_data = pd.merge(data_filtered, seird_data, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "merged_data.dropna(inplace=True)\n",
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target variable and make it stationary\n",
    "target = \"covidOccupiedMVBeds\"\n",
    "seasonal_period = 7\n",
    "auto_stationary = AutoStationaryTransformer(seasonal_period=seasonal_period)\n",
    "data_stat = auto_stationary.fit_transform(merged_data[[target]], freq=\"D\")\n",
    "merged_data[target] = data_stat.values\n",
    "\n",
    "# Ensure the index is a DateTimeIndex\n",
    "merged_data.index = pd.to_datetime(merged_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 13:13:51,408 - INFO - Data ranges from 2020-05-01 00:00:00 to 2021-05-31 00:00:00 (395 days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 69.95%, Validation: 19.95%, Test: 10.10%\n",
      "Train: 277 samples, Validation: 79 samples, Test: 40 samples\n",
      "Max date in train: 2021-02-01 00:00:00, Min date in validation: 2021-02-02 00:00:00, Max date in test: 2021-05-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Filter data between the specified dates\n",
    "start_date = \"2020-05-01\"\n",
    "end_date = \"2021-05-31\"\n",
    "merged_data = merged_data[start_date:end_date]\n",
    "\n",
    "min_date = merged_data.index.min()\n",
    "max_date = merged_data.index.max()\n",
    "\n",
    "# Calculate the range of dates\n",
    "date_range = max_date - min_date\n",
    "logging.info(f\"Data ranges from {min_date} to {max_date} ({date_range.days} days)\")\n",
    "\n",
    "# Calculate split points\n",
    "total_days = date_range.days\n",
    "train_end = min_date + pd.Timedelta(days=int(total_days * 0.70))\n",
    "val_end = train_end + pd.Timedelta(days=int(total_days * 0.20))\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train = merged_data[merged_data.index <= train_end]\n",
    "val = merged_data[(merged_data.index > train_end) & (merged_data.index <= val_end)]\n",
    "test = merged_data[merged_data.index > val_end]\n",
    "\n",
    "# Calculate the percentage of dates in each dataset\n",
    "total_sample = len(merged_data)\n",
    "train_sample = len(train) / total_sample * 100\n",
    "val_sample = len(val) / total_sample * 100\n",
    "test_sample = len(test) / total_sample * 100\n",
    "\n",
    "print(f\"Train: {train_sample:.2f}%, Validation: {val_sample:.2f}%, Test: {test_sample:.2f}%\")\n",
    "print(f\"Train: {len(train)} samples, Validation: {len(val)} samples, Test: {len(test)} samples\")\n",
    "print(f\"Max date in train: {train.index.max()}, Min date in validation: {val.index.min()}, Max date in test: {test.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dates: (Timestamp('2020-05-01 00:00:00'), Timestamp('2021-02-01 00:00:00')), Val dates: (Timestamp('2021-02-02 00:00:00'), Timestamp('2021-04-21 00:00:00')), Test dates: (Timestamp('2021-04-22 00:00:00'), Timestamp('2021-05-31 00:00:00'))\n"
     ]
    }
   ],
   "source": [
    "train_dates = (train.index.min(), train.index.max())\n",
    "val_dates = (val.index.min(), val.index.max())\n",
    "test_dates = (test.index.min(), test.index.max())\n",
    "\n",
    "print(f\"Train dates: {train_dates}, Val dates: {val_dates}, Test dates: {test_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
